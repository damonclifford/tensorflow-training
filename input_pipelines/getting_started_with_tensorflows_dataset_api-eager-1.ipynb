{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow's `Dataset` API with eager execution\n",
    "\n",
    "This notebook contains examples on how to build simple input pipelines with TensorFlow's [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data/) API. The examples are based on contructing `Dataset` objects from Numpy arrays in memory, which is intended to be used only with very small datasets as it can be considerably inefficient.\n",
    "\n",
    "More info can be found on the session [Importing Data](https://www.tensorflow.org/guide/datasets) on TensorFlow's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake data\n",
    "nsamples = 20\n",
    "nfeatures = 4\n",
    "x_numpy = np.random.random((nsamples, nfeatures))\n",
    "y_numpy = x_numpy.sum(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a `Dataset` object\n",
    "# With eager execution there is not need to create the iterator object\n",
    "# to iterate over the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_numpy, y_numpy))\n",
    "dataset = dataset.shuffle(10)\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_classes: (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      "output_shapes:  (TensorShape([Dimension(None), Dimension(4)]), TensorShape([Dimension(None)]))\n",
      "output_types:   (tf.float64, tf.int64)\n"
     ]
    }
   ],
   "source": [
    "# This properties of a Dataset instance allow you to inspect\n",
    "# the types, classes (they are allways Tensor though) and\n",
    "# shapes of the components of a dataset element.\n",
    "print('output_classes:', dataset.output_classes)\n",
    "print('output_shapes: ', dataset.output_shapes)\n",
    "print('output_types:  ', dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sarafael/software/anaconda3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "features: [[0.57450237 0.21575314 0.78712689 0.28127561]]  |  label: [1]\n",
      "features: [[0.01352067 0.13658407 0.30805905 0.83829867]]  |  label: [1]\n",
      "features: [[0.78760446 0.85005774 0.92827435 0.78870465]]  |  label: [3]\n",
      "features: [[0.32542749 0.54024076 0.85206354 0.92061661]]  |  label: [2]\n",
      "features: [[0.12255501 0.39644084 0.98641667 0.07655477]]  |  label: [1]\n",
      "features: [[0.62194791 0.34082038 0.06981713 0.48575887]]  |  label: [1]\n",
      "features: [[0.13269982 0.32203321 0.08099787 0.20184865]]  |  label: [0]\n",
      "features: [[0.44447412 0.31827045 0.87963864 0.20281922]]  |  label: [1]\n",
      "features: [[0.45134301 0.50320016 0.32430183 0.88046682]]  |  label: [2]\n",
      "features: [[0.30191061 0.65322297 0.54011029 0.73059632]]  |  label: [2]\n",
      "features: [[0.12390366 0.34119404 0.79383614 0.70713711]]  |  label: [1]\n",
      "features: [[0.78364559 0.009132   0.03656106 0.09885238]]  |  label: [0]\n",
      "features: [[0.19541322 0.74802709 0.36409407 0.2574914 ]]  |  label: [1]\n",
      "features: [[0.02825905 0.24807665 0.58604084 0.79782747]]  |  label: [1]\n",
      "features: [[0.29867272 0.05387456 0.17732204 0.2229413 ]]  |  label: [0]\n",
      "features: [[0.98564307 0.26921802 0.52344173 0.63675444]]  |  label: [2]\n",
      "features: [[0.83631338 0.61832159 0.99789804 0.30505583]]  |  label: [2]\n",
      "features: [[0.23827784 0.56607259 0.70679336 0.2020813 ]]  |  label: [1]\n",
      "features: [[0.27205977 0.84139521 0.27271601 0.15365334]]  |  label: [1]\n",
      "features: [[0.9554669  0.47081111 0.57550431 0.42931029]]  |  label: [2]\n"
     ]
    }
   ],
   "source": [
    "for features, label in dataset:\n",
    "    print('features: %s  |  label: %s' % (features.numpy(), label.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With eager execution `tf.errors.OutOfRangeError` is no longer raised when the iterator reaches the end of the dataset (with all the repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Dataset` object can be created also from Tensor objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = tf.random.uniform([nsamples, nfeatures])\n",
    "y_tensor = tf.cast(tf.reduce_sum(x_tensor, axis=1), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_tensor, y_tensor))\n",
    "dataset = dataset.shuffle(10)\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[0.9508163  0.388245   0.10347426 0.17912197]]  |  label: [1]\n",
      "features: [[0.7252369  0.0251298  0.05889571 0.60462415]]  |  label: [1]\n",
      "features: [[0.9087721  0.6278064  0.93858564 0.76789796]]  |  label: [3]\n",
      "features: [[0.3583138  0.07513583 0.38090575 0.20150185]]  |  label: [1]\n",
      "features: [[0.99589133 0.70830715 0.15883005 0.05250812]]  |  label: [1]\n",
      "features: [[0.7065567  0.55317616 0.39521015 0.6568639 ]]  |  label: [2]\n",
      "features: [[0.18450737 0.6289238  0.9752656  0.44455767]]  |  label: [2]\n",
      "features: [[0.4589064  0.9014076  0.9669143  0.45641887]]  |  label: [2]\n",
      "features: [[0.37846136 0.47982407 0.23239541 0.6978121 ]]  |  label: [1]\n",
      "features: [[0.02581453 0.24033892 0.07214499 0.02725887]]  |  label: [0]\n",
      "features: [[0.07872832 0.4945264  0.11889136 0.73061514]]  |  label: [1]\n",
      "features: [[0.7008077  0.8412634  0.46224904 0.30928707]]  |  label: [2]\n",
      "features: [[0.26136672 0.07456124 0.17268598 0.19749177]]  |  label: [0]\n",
      "features: [[0.20625508 0.11120594 0.42264736 0.8693373 ]]  |  label: [1]\n",
      "features: [[0.55424666 0.67967653 0.79629743 0.90002465]]  |  label: [2]\n",
      "features: [[0.67826974 0.2674036  0.45824313 0.18431985]]  |  label: [1]\n",
      "features: [[0.7738762  0.35178876 0.16163218 0.10848665]]  |  label: [1]\n",
      "features: [[0.59836674 0.15288305 0.9269341  0.5150356 ]]  |  label: [2]\n",
      "features: [[0.48964953 0.19862628 0.31453753 0.17123926]]  |  label: [1]\n",
      "features: [[0.6277977  0.7514157  0.15567362 0.75067055]]  |  label: [2]\n"
     ]
    }
   ],
   "source": [
    "for features, label in dataset:\n",
    "    print('features: %s  |  label: %s' % (features.numpy(), label.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) embeds the features and labels arrays in your TensorFlow graph as `tf.constant` operations. This works well for a small dataset, but wastes memory because the contents of the array will be copied multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data transformations to the pipeline\n",
    "\n",
    "Lest's say that for our problem it is beneficial to center the features between -0.5 and 0.5. Also, we would like to transform the labels from integers to one-hot encoded. This is can be donde with `Dataset`'s method `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following transformations are quite simple and can be done\n",
    "# on a single function, but we will use two different functions\n",
    "# to show how operations can be pipelined.\n",
    "\n",
    "def center(*row):\n",
    "    features = row[0] - 0.5\n",
    "    label = row[1]\n",
    "    return features, label\n",
    "\n",
    "def make_on_hot_labels(features, label):\n",
    "    return features, tf.one_hot(label, 4)\n",
    "\n",
    "# simpler with `dataset = dataset.filter(lambda f, l: tf.equal(l, 1))`\n",
    "def filter_labels(features, label):\n",
    "    return tf.equal(label, 1)\n",
    "\n",
    "# simpler with `dataset = dataset.filter(lambda f, l: tf.greater(f[0], 0)`\n",
    "def filter_features(features, label):\n",
    "    return tf.greater(features[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[ 0.07450237 -0.28424686  0.28712689 -0.21872439]]  |  label: [[0. 1. 0. 0.]]\n",
      "features: [[ 0.12194791 -0.15917962 -0.43018287 -0.01424113]]  |  label: [[0. 1. 0. 0.]]\n",
      "features: [[ 0.12194791 -0.15917962 -0.43018287 -0.01424113]]  |  label: [[0. 1. 0. 0.]]\n",
      "features: [[ 0.07450237 -0.28424686  0.28712689 -0.21872439]]  |  label: [[0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_numpy, y_numpy))\n",
    "dataset = dataset.filter(filter_labels)\n",
    "dataset = dataset.map(center)\n",
    "dataset = dataset.filter(filter_features)\n",
    "dataset = dataset.map(make_on_hot_labels)\n",
    "dataset = dataset.shuffle(150)\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.repeat(2)\n",
    "\n",
    "for features, label in dataset:\n",
    "    print('features: %s  |  label: %s' % (features.numpy(), label.numpy()))\n",
    "    \n",
    "# Note here that maybe nothing is printed. That's because the rando data is\n",
    "# such that the two filter operations will filter it out completely. In that\n",
    "# case you may generate again the data and run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving names to `Dataset` components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_classes: {'features': <class 'tensorflow.python.framework.ops.Tensor'>, 'label': <class 'tensorflow.python.framework.ops.Tensor'>}\n",
      "output_shapes:  {'features': TensorShape([Dimension(4)]), 'label': TensorShape([])}\n",
      "output_types:   {'features': tf.float64, 'label': tf.int64}\n"
     ]
    }
   ],
   "source": [
    "named_dataset = tf.data.Dataset.from_tensor_slices({'features': x_numpy, 'label': y_numpy})\n",
    "print('output_classes:', named_dataset.output_classes)\n",
    "print('output_shapes: ', named_dataset.output_shapes)\n",
    "print('output_types:  ', named_dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(row):\n",
    "    features = row['features'] - 0.5\n",
    "    label = row['label']\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[-0.19808939  0.15322297  0.04011029  0.23059632]]  |  label: [2]\n",
      "features: [[ 0.28364559 -0.490868   -0.46343894 -0.40114762]]  |  label: [0]\n",
      "features: [[-0.36730018 -0.17796679 -0.41900213 -0.29815135]]  |  label: [0]\n",
      "features: [[-0.47174095 -0.25192335  0.08604084  0.29782747]]  |  label: [1]\n",
      "features: [[-0.20132728 -0.44612544 -0.32267796 -0.2770587 ]]  |  label: [0]\n",
      "features: [[ 0.33631338  0.11832159  0.49789804 -0.19494417]]  |  label: [2]\n",
      "features: [[ 0.12194791 -0.15917962 -0.43018287 -0.01424113]]  |  label: [1]\n",
      "features: [[-0.48647933 -0.36341593 -0.19194095  0.33829867]]  |  label: [1]\n",
      "features: [[0.28760446 0.35005774 0.42827435 0.28870465]]  |  label: [3]\n",
      "features: [[ 0.48564307 -0.23078198  0.02344173  0.13675444]]  |  label: [2]\n",
      "features: [[-0.37609634 -0.15880596  0.29383614  0.20713711]]  |  label: [1]\n",
      "features: [[-0.30458678  0.24802709 -0.13590593 -0.2425086 ]]  |  label: [1]\n",
      "features: [[-0.17457251  0.04024076  0.35206354  0.42061661]]  |  label: [2]\n",
      "features: [[-0.37744499 -0.10355916  0.48641667 -0.42344523]]  |  label: [1]\n",
      "features: [[ 0.4554669  -0.02918889  0.07550431 -0.07068971]]  |  label: [2]\n",
      "features: [[-0.05552588 -0.18172955  0.37963864 -0.29718078]]  |  label: [1]\n",
      "features: [[ 0.07450237 -0.28424686  0.28712689 -0.21872439]]  |  label: [1]\n",
      "features: [[-0.22794023  0.34139521 -0.22728399 -0.34634666]]  |  label: [1]\n",
      "features: [[-0.26172216  0.06607259  0.20679336 -0.2979187 ]]  |  label: [1]\n",
      "features: [[-0.04865699  0.00320016 -0.17569817  0.38046682]]  |  label: [2]\n"
     ]
    }
   ],
   "source": [
    "named_dataset = tf.data.Dataset.from_tensor_slices({'features': x_numpy, 'label': y_numpy})\n",
    "named_dataset = named_dataset.map(center)\n",
    "named_dataset = named_dataset.shuffle(150)\n",
    "named_dataset = named_dataset.batch(1)   # batch size\n",
    "named_dataset = named_dataset.repeat(1)  # number of epochs\n",
    "\n",
    "for features, label in named_dataset:\n",
    "    print('features: %s  |  label: %s' % (features.numpy(), label.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
